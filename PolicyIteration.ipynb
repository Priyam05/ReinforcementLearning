{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import policy and value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mappings = {\n",
    "    0: '\\u2191', # UP\n",
    "    1: '\\u2192', # RIGHT\n",
    "    2: '\\u2193', # DOWN\n",
    "    3: '\\u2190', # LEFT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins=0\n",
    "    total_reward=0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state=environment.reset()\n",
    "        terminated=False\n",
    "        \n",
    "        while not terminated:\n",
    "            #Choose best action according to the policy\n",
    "            action=np.argmax(policy[state])\n",
    "            \n",
    "            #Perform the action in the environment\n",
    "            next_state, rewards, terminated, info=environment.step(action)\n",
    "            \n",
    "            #Add the reward to the total reward count\n",
    "            total_reward+=rewards\n",
    "            \n",
    "            #Update the current state\n",
    "            state=next_state\n",
    "            \n",
    "            #Check if episode is terminated and the reward is achieved....Note that reward is 1 at the goal\n",
    "            if terminated and rewards==1.0:\n",
    "                wins+=1\n",
    "                \n",
    "    avg_reward=total_reward/n_episodes\n",
    "    \n",
    "    return wins, total_reward, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy Iteration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for fun if you want to check how the environment works, in the below equation 1st value is current state and 2nd value \n",
    "#is the action taken. Then environment gives the state_probablity, next_state, reward, terminated\n",
    "environment.P[50][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for policy evaluation. Note this will be done using Bellman Expected Equation\n",
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    #theta: Convergence factor. If the change in value function for all the states is below theta then we are done\n",
    "    \n",
    "    print('Evaluating the policy. Max iterations set is:{}'.format(max_iterations))\n",
    "    \n",
    "    #Initialise the value function\n",
    "    V=np.zeros(environment.nS) #Every policy needs to be evaluated from scratch\n",
    "    \n",
    "    print('Value function initialised successfully')\n",
    "    \n",
    "    #Initialise a counter to monitor the number of evaluations for a policy\n",
    "    evaluation_iterations=0\n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        \n",
    "        #Initialise delta to monitor the convergence\n",
    "        delta=0\n",
    "    \n",
    "    \n",
    "        for state in range(environment.nS):\n",
    "            v=0 #This will be used to maintain the value of a state in an iteration\n",
    "\n",
    "            #Check for all possible actions using the current policy\n",
    "            for action, action_probablity in enumerate(policy[state]):\n",
    "\n",
    "                #Now get the next_state probablity, next_state, reward, terminated values from environment\n",
    "                for state_probablity, next_state, reward, terminated in environment.P[state][action]:\n",
    "\n",
    "                    #Bellman Expected Equation\n",
    "                    v+=action_probablity*(reward+(state_probablity*discount_factor*V[next_state]))\n",
    "                    \n",
    "            #Capture the change in value of the state Note: We will be maintaining only the maximum delta of all the states\n",
    "            delta=max(delta,abs(V[state]-v))       \n",
    "\n",
    "            V[state]=v  #Update the state value function\n",
    "        \n",
    "        #increment the counter\n",
    "        evaluation_iterations+=1\n",
    "        \n",
    "               \n",
    "        #Early stopping\n",
    "        if(delta<theta):\n",
    "            print('Policy evaluated in {} iterations'.format(evaluation_iterations))\n",
    "            print('Final Value function')\n",
    "            print(V)\n",
    "            return V  \n",
    "        \n",
    "\n",
    "    \n",
    "                    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_q_value(environment, state, state_value_fn, discount_factor=1.0):\n",
    "    #Here we will be calculating the q-values for given state with every action\n",
    "    \n",
    "    #initialise the q-values \n",
    "    q_values= np.zeros(environment.nA)\n",
    "    \n",
    "    #Note: Q-Value= Reward + DiscountFactor*EnvironmentProbablity*ValueFn\n",
    "    \n",
    "    for action in range(environment.nA):\n",
    "        \n",
    "        #For each action, environment can take us to any state based on transition model\n",
    "        for state_probablity, next_state, reward, terminated in environment.P[state][action]:\n",
    "            \n",
    "            #Now calculate q-value\n",
    "            q_values[action]+= reward+(state_probablity*discount_factor*state_value_fn[next_state])\n",
    "                                       \n",
    "    return q_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    \n",
    "    #Initialise a policy matrix with uniform distribution\n",
    "    policy=np.ones((environment.nS,environment.nA))/environment.nA\n",
    "    \n",
    "    \n",
    "    #Initialise a variable to store the number of policies evaluated\n",
    "    evaluated_policies=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        \n",
    "        evaluated_policies+=1  \n",
    "        \n",
    "        #Evaluate the current policy\n",
    "        state_value_fn=policy_evaluation(policy, environment)\n",
    "        \n",
    "        print('Value function after {}th policy evaluation:'.format(evaluated_policies))\n",
    "        print(state_value_fn)\n",
    "        \n",
    "        #Initialise a boolean to track the stable policy\n",
    "        stable_policy=True\n",
    "                \n",
    "        #Now we need to act greedily on this evaluated policy. For that we will calculate the q-values for every state\n",
    "        # and take max. We will calculate the q-values using the evaluated policy for each state\n",
    "               \n",
    "        for state in range(environment.nS):\n",
    "            \n",
    "            #Get the action using current policy\n",
    "            current_action=np.argmax(policy[state])\n",
    "            print('Current policy selects action:{0} for state{1}'.format(current_action, state))\n",
    "            \n",
    "            #Calculate the q-values for this state\n",
    "            q_values= cal_q_value(environment, state, state_value_fn)\n",
    "            print('Q-values for state:{} is:'.format(state))\n",
    "            print(q_values)\n",
    "            \n",
    "            #Now act greedily to get the best action\n",
    "            best_action=np.argmax(q_values)\n",
    "            print('Greedy Policy selects action:{0} for state{1}'.format(best_action, state))\n",
    "              \n",
    "            #If the best action is not same as current action, then we need to change the policy\n",
    "            if (current_action!=best_action):\n",
    "                stable_policy=False  #Set the policy to False as we need to iterate further\n",
    "                 \n",
    "                 \n",
    "            #Update the policy for the state\n",
    "            policy[state]=np.eye(environment.nA)[best_action] #This will add the value 1 at the index of best_action\n",
    "            \n",
    "        if stable_policy:\n",
    "            return policy, state_value_fn\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lets the create the optimum policy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the environment\n",
    "environment = gym.make('FrozenLake8x8-v0')\n",
    "print('Environment created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, state_value_fn= policy_iteration(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins, total_reward, average_reward= play_episodes(environment,1000, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
