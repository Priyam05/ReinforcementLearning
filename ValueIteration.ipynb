{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import policy and value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mappings = {\n",
    "    0: '\\u2191', # UP\n",
    "    1: '\\u2192', # RIGHT\n",
    "    2: '\\u2193', # DOWN\n",
    "    3: '\\u2190', # LEFT\n",
    "}2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins=0\n",
    "    total_reward=0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state=environment.reset()\n",
    "        terminated=False\n",
    "        \n",
    "        while not terminated:\n",
    "            #Choose best action according to the policy\n",
    "            action=np.argmax(policy[state])\n",
    "            \n",
    "            #Perform the action in the environment\n",
    "            next_state, rewards, terminated, info=environment.step(action)\n",
    "            \n",
    "            #Add the reward to the total reward count\n",
    "            total_reward+=rewards\n",
    "            \n",
    "            #Update the current state\n",
    "            state=next_state\n",
    "            \n",
    "            #Check if episode is terminated and the reward is achieved....Note that reward is 1 at the goal\n",
    "            if terminated and rewards==1.0:\n",
    "                wins+=1\n",
    "                \n",
    "    avg_reward=total_reward/n_episodes\n",
    "    \n",
    "    return wins, total_reward, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Iteration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for fun if you want to check how the environment works, in the below equation 1st value is current state and 2nd value \n",
    "#is the action taken. Then environment gives the state_probablity, next_state, reward, terminated\n",
    "environment.P[50][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    \n",
    "    #Initialise the state value function\n",
    "    state_values=np.zeros(environment.nS)\n",
    "    \n",
    "    #Initialise a counter\n",
    "    evaluation_iterations=0\n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        evaluation_iterations+=1\n",
    "        \n",
    "        #Initialise delta for early stopping\n",
    "        delta=0\n",
    "        \n",
    "        for state in range(environment.nS):\n",
    "            \n",
    "            q_values=cal_q_values(environment, state, state_values)\n",
    "            \n",
    "            #We will get four values for this state. We wil pick the max out of it\n",
    "            best_action_value= np.max(q_values)\n",
    "            \n",
    "            delta= max(delta, abs(state_values[state]-best_action_value))\n",
    "            \n",
    "            #Update the state value function\n",
    "            state_values[state]=best_action_value\n",
    "            \n",
    "        #Early Stopping\n",
    "        if(delta<theta):\n",
    "            print('Value iteration converged on {}th iteration.'.format(evaluation_iterations))\n",
    "            print(state_values)\n",
    "            break\n",
    "            \n",
    "    #Now we need to find the optimal policy using the State Value function\n",
    "    policy=np.zeros((environment.nS, environment.nA))\n",
    "    \n",
    "    for state in range(environment.nS):\n",
    "        q_values=cal_q_values(environment, state, state_values)\n",
    "        \n",
    "        #Get the best action\n",
    "        best_action=np.argmax(q_values)\n",
    "        \n",
    "        #Update the policy\n",
    "        policy[state]=np.eye(environment.nA)[best_action]\n",
    "        \n",
    "    print('Final policy is:')\n",
    "    print(policy)\n",
    "    return policy, state_values\n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_q_values(environment, state, state_value_fn, discount_factor=1.0):\n",
    "    #Here we will be calculating the q-values for given state with every action\n",
    "    \n",
    "    #initialise the q-values \n",
    "    q_values= np.zeros(environment.nA)\n",
    "    \n",
    "    #Note: Q-Value= Reward + DiscountFactor*EnvironmentProbablity*ValueFn\n",
    "    \n",
    "    for action in range(environment.nA):\n",
    "        \n",
    "        #For each action, environment can take us to any state based on transition model\n",
    "        for state_probablity, next_state, reward, terminated in environment.P[state][action]:\n",
    "            \n",
    "            #Now calculate q-value\n",
    "            q_values[action]+= reward+(state_probablity*discount_factor*state_value_fn[next_state])\n",
    "                                       \n",
    "    return q_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lets the create the optimum policy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created.\n"
     ]
    }
   ],
   "source": [
    "#Create the environment\n",
    "environment = gym.make('FrozenLake8x8-v0')\n",
    "print('Environment created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged on 968th iteration.\n",
      "[2.99999999 2.99999999 2.99999999 2.99999999 2.99999999 2.99999999\n",
      " 2.99999999 2.99999999 2.99999999 2.99999999 2.99999999 2.99999999\n",
      " 2.99999999 2.99999999 2.99999999 2.99999999 2.99999998 2.93460489\n",
      " 2.77929154 0.         2.56985302 2.83869488 2.94623162 2.99999999\n",
      " 2.99999997 2.80381469 2.40326974 1.42471131 1.8708642  0.\n",
      " 2.83403282 3.         2.99999996 2.47683921 1.62670298 0.\n",
      " 1.61802826 1.8335677  2.55586684 3.         2.99999996 0.\n",
      " 0.         0.50412238 1.14965288 1.326808   0.         3.\n",
      " 2.99999995 0.         0.58402039 0.36271426 0.         0.99720343\n",
      " 0.         3.         2.99999995 2.19467343 1.38934691 0.\n",
      " 0.83240114 1.66480229 2.33240114 0.        ]\n",
      "Final policy is:\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "policy, state_value_fn= value_iteration(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins, total_reward, average_reward= play_episodes(environment,1000, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
